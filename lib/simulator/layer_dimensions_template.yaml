---
##############################################################################
# Copyright (C) 2025 Joel Klein
# All Rights Reserved
#
# This work is licensed under the terms described in the LICENSE file
# found in the root directory of this source tree.
##############################################################################
# Template for layer dimensions in YAML format
# Format for each layer:
#   type: "Conv2D"/"Dense"/"MatMul"
#   output_dim: number of output channels/features/rows
#   input_dim: input dimension (kernel_channels*kernel_height*kernel_width for Conv2D, features for Dense, input columns for MatMul)
#   mvm_invocations: "output_height*output_width" for Conv2D layers, number of output columns for MatMul (omit for Dense)
#   repeat_factor: (optional) number of times to repeat the layer (e.g., number of heads in a transformer block for MatMul)
#
# For convolutional layers:
# - output_dim = number of output filters
# - input_dim = kernel_channels * kernel_height * kernel_width
# - mvm_invocations = "output_height*output_width"
#
# For fully connected layers:
# - output_dim = number of output features
# - input_dim = number of input features
# - mvm_invocations not needed (default: 1)
#
# For matrix multiplication layers (m x k @ k x n = m x n)
# - output_dim = number of input/output rows (m)
# - input_dim = number of input columns (k)
# - mvm_invocations = number of output columns (n)
# - repeat_factor = number of times this operation is repeated (e.g., heads in attention)
#
# For Multi-Head Attention (MHA) layers specifically:
# - QKV projection: Dense layer with mvm_invocations = sequence_length (N MVMs for N tokens)
# - Q@K^T computation: MatMul with mvm_invocations = sequence_length, repeat_factor = num_heads (K is mapped to the crossbar and Q is decomposed into multiple rows as input column vectors)
# - Attention@V computation: MatMul with mvm_invocations = sequence_length, repeat_factor = num_heads (V^T is mapped to the crossbar and Attention is decomposed into multiple rows as input column vectors)
# - Output projection: Dense layer with mvm_invocations = sequence_length (N MVMs for N tokens)
# - MLP layers: Dense layers with mvm_invocations = sequence_length (N MVMs for N tokens)
layer_dimensions:
  # Example for a CNN network

  # Convolutional layers
  - type: Conv2D
    output_dim: 64
    input_dim: 3*3*3  # 3 channels, 3x3 kernel
    mvm_invocations: 224*224  # Output size after convolution
  - type: Conv2D
    output_dim: 64
    input_dim: 64*3*3  # 64 channels, 3x3 kernel
    mvm_invocations: 224*224  # Same padding

  # After pooling (e.g., 2x2 pooling reduces spatial dimensions)
  - type: Conv2D
    output_dim: 128
    input_dim: 64*3*3  # 64 channels, 3x3 kernel
    mvm_invocations: 112*112
  - type: Conv2D
    output_dim: 128
    input_dim: 128*3*3  # 128 channels, 3x3 kernel
    mvm_invocations: 112*112

  # After more pooling
  - type: Conv2D
    output_dim: 256
    input_dim: 128*3*3  # 128 channels, 3x3 kernel
    mvm_invocations: 56*56
  - type: Conv2D
    output_dim: 256
    input_dim: 256*3*3  # 256 channels, 3x3 kernel
    mvm_invocations: 56*56

  # After more pooling
  - type: Conv2D
    output_dim: 512
    input_dim: 256*3*3  # 256 channels, 3x3 kernel
    mvm_invocations: 28*28
  - type: Conv2D
    output_dim: 512
    input_dim: 512*3*3  # 512 channels, 3x3 kernel
    mvm_invocations: 28*28

  # Fully connected layers
  - type: Dense
    output_dim: 4096
    input_dim: 512*7*7  # Flattened from 512×7×7
  - type: Dense
    output_dim: 4096
    input_dim: 4096
  - type: Dense
    output_dim: 1000
    input_dim: 4096

  # MatMul layers (e.g. inside a transformer block) (m x k @ k x n = m x n)
  - type: MatMul
    output_dim: 64  # m
    input_dim: 128  # k
    mvm_invocations: 50  # n (number of independent MVM executions)
    repeat_factor: 12  # Number of times this layer is repeated (number of heads in a transformer block)

  # Example Multi-Head Attention block (12 heads, embed_dim=768, sequence_length=50)
  # QKV projection
  - type: Dense
    output_dim: 2304  # embed_dim * 3 = 768 * 3 (combined Q,K,V)
    input_dim: 768  # embed_dim
    mvm_invocations: 50  # sequence_length (N MVMs for N tokens)
  # Q@K^T computation (GeMM: N x d_k @ d_k x N = N x N)
  # Store K on crossbars, use Q as input
  - type: MatMul
    output_dim: 50  # sequence_length (rows of K)
    input_dim: 64  # head_dim (d_k = embed_dim / num_heads = 768 / 12) (columns of K)
    mvm_invocations: 50  # sequence_length (rows of Q)
    repeat_factor: 12  # num_heads (repeated h times)
  # Attention@V computation (GeMM: d_k x N @ N x N = d_k x N)
  # Store V^T on crossbars, use Attention as input
  - type: MatMul
    output_dim: 64  # head_dim (d_k = embed_dim / num_heads = 768 / 12) (rows of V^T/output)
    input_dim: 50  # sequence_length (columns of V^T)
    mvm_invocations: 50  # sequence_length (N MVMs for N tokens) (rows of Attention)
    repeat_factor: 12  # num_heads (repeated h times)
  # Output projection
  - type: Dense
    output_dim: 768  # embed_dim
    input_dim: 768  # embed_dim (concatenated from all heads)
    mvm_invocations: 50  # sequence_length (N MVMs for N tokens)
  # MLP layers in transformer
  - type: Dense
    output_dim: 3072  # mlp_dim (usually 4 * embed_dim)
    input_dim: 768  # embed_dim
    mvm_invocations: 50  # sequence_length (N MVMs for N tokens)
  - type: Dense
    output_dim: 768  # embed_dim
    input_dim: 3072  # mlp_dim
    mvm_invocations: 50  # sequence_length (N MVMs for N tokens)
# Note: The mvm_invocations field is used to calculate the number of MVM executions.
# It is particularly relevant for Conv2D, MatMul, and MHA layers where GeMMs are divided into multiple MVM executions.
#
# Weight Mapping Modes (Nw = ceil(w_bit / cell_resolution)):
#   m (output_dim) maps to crossbar columns (M), n (input_dim) maps to crossbar rows (N)
#   - offset (of): +1 column → num_cols = m * Nw + 1
#   - differential-column (dc): 2 columns per weight → num_cols = m * Nw * 2
#   - differential-row (dr): 2 rows per weight → num_rows = n * 2
#
# Cost formulas:
#   num_mvm_writes = ceil(num_cols / crossbar_M) * ceil(num_rows / crossbar_N)
#   num_mvm_executes = mvm_invocations * num_mvm_writes * ceil(a_bit / input_resolution)
#   total_latency = repeat_factor * (num_mvm_writes * write_latency + num_mvm_executes * execute_latency)
#
# For different layer types:
# - Conv2D layers: mvm_invocations is derived from the spatial dimensions of the output feature map
# - Dense layers (standard): mvm_invocations = 1 since there is no spatial dimension
# - Dense layers (in MHA): mvm_invocations = sequence_length for sequential token processing
# - MatMul layers (in MHA): mvm_invocations represents GeMM decomposition, repeat_factor represents heads
#
# For Multi-Head Attention specifically:
# 1. QKV projection: Dense(768→2304), mvm_invocations=N (process N tokens sequentially)
# 2. Q@K^T computation: Store K on crossbars, use Q as input, mvm_invocations=N, repeat_factor=h
# 3. Attention@V: Store V^T on crossbars, use Attention as input, mvm_invocations=N, repeat_factor=h
# 4. Output projection: Dense(768→768), mvm_invocations=N (process N tokens sequentially)
# 5. MLP layers: Dense layers with mvm_invocations=N (process N tokens sequentially)
#
# Key insight: Both MHA MatMul operations use mvm_invocations=sequence_length by:
# - Q@K^T: Store K(N×d_k) on crossbars, input Q^T(N×d_k) → output(N×N)
# - Attention@V: Store V^T(d_k×N) on crossbars, input Attention^T(N×N) → output(N×d_k)
# Note: The calculation on the crossbar the multiplication is transformed before offloading:
#       - Q@K^T = ((Q@K^T)^T)^T = ((K^T)^T@Q^T)^T = (K@Q^T)^T = (output^T)^T = output
#       - Attention@V = ((Attention@V)^T)^T = (V^T@Attention^T)^T = (output^T)^T = output

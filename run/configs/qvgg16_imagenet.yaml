---
##############################################################################
# Copyright (C) 2025 Joel Klein
# All Rights Reserved
#
# This work is licensed under the terms described in the LICENSE file
# found in the root directory of this source tree.
##############################################################################
# CIM-AQ Full Workflow Configuration Template
# Copy this file and modify the values as needed
# Model Configuration
models:
  quant_model: qvgg16  # Quantized model architecture
  fp32_model: custom_vgg16  # Full precision model architecture
# Dataset Configuration  
datasets:
  small:
    name: imagenet100  # Small dataset name (for RL search)
    root: data/imagenet100  # Path relative to repo root, or absolute path
  large:
    name: imagenet  # Large dataset name (for final training)
    root: data/imagenet  # Path relative to repo root, or absolute path
    enabled: true  # Whether to run the large dataset stage
# Quantization Configuration
quantization:
  max_accuracy_drop: 1.0  # Maximum allowed accuracy drop in percentage points
  min_bit: 2  # Minimum bit-width
  max_bit: 8  # Maximum bit-width
  force_first_last_layer: true  # Force first and last layers to high precision
  consider_cell_resolution: true  # Whether to consider cell resolution for weight bit widths
# Training Configuration
training:
  # Learning rates for different stages and datasets
  learning_rates:
    small_dataset:
      fp32_pretraining: 0.005  # Learning rate for FP32 model fine-tuning on small dataset
      int8_pretraining: 0.00005  # Learning rate for INT8 model pre-fine-tuning on small dataset  
      mixed_precision_finetuning: 0.001  # Learning rate for final mixed precision fine-tuning on small dataset
      rl_finetune: 0.001  # Learning rate for fine-tuning during RL search
    large_dataset:
      fp32_pretraining: 0.00005  # Learning rate for FP32 model fine-tuning on large dataset
      int8_pretraining: 0.00001  # Learning rate for INT8 model pre-fine-tuning on large dataset
      mixed_precision_finetuning: 0.0005  # Learning rate for final mixed precision fine-tuning on large dataset
  rl:
    train_episodes: 600  # Number of training episodes for RL search
  small_dataset:
    fp32_epochs: 30  # Number of epochs for FP32 model fine-tuning
    int8_epochs: 30  # Number of epochs for 8-bit pre-fine-tuning
    search_finetune_epochs: 3  # Number of epochs for fine-tuning during search
    final_finetune_epochs: 30  # Number of epochs for final fine-tuning
  large_dataset:
    fp32_epochs: 30  # Number of epochs for FP32 model fine-tuning
    int8_epochs: 30  # Number of epochs for 8-bit pre-fine-tuning
    final_finetune_epochs: 30  # Number of epochs for final fine-tuning
# Device Configuration
device:
  gpu_id: '1'  # GPU ID(s) for CUDA_VISIBLE_DEVICES (e.g., "0", "0,1", "0,1,2,3")
# DataLoader Configuration
dataloader:
  batch_size: 256  # Batch size for training/evaluation
  num_workers: 32  # Number of DataLoader workers
# Output Configuration
output:
  prefix: both_constraints  # Optional prefix for output folders
# Logging Configuration
logging:
  wandb:
    enable: true  # Enable Weights & Biases logging
    project: cim-aq-quantization  # W&B project name
# Space Management Configuration
# These options help manage disk space during training, especially useful in CI environments
# or when working with limited storage. They control automatic cleanup of intermediate files.
space_management:
  enable_cleanup: false  # Enable automatic cleanup of intermediate files during training
  cleanup_frequency: end  # When to perform cleanup: 'step' (after each training step), 'stage' (after each workflow stage), 'end' (only at workflow end)
  cleanup_targets: checkpoints  # What to clean up - can be single string or list:
    # Options: "checkpoints", "cache", "logs"
    # Use "all" for everything, or list like: ["checkpoints", "cache"]
